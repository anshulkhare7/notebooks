{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e0638b-4a9a-4350-bc42-7d178b5c536d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "8deca410-fce1-4e30-96eb-622f1b79e92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed filelock-3.16.1 fsspec-2024.9.0 huggingface-hub-0.25.1 mpmath-1.3.0 networkx-3.3 safetensors-0.4.5 sympy-1.13.3 tokenizers-0.20.0 torch-2.4.1 transformers-4.45.1\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2 pdfplumber spacy nltk transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "a1d3cd31-aa7a-4091-89f3-108241df3133",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from IPython.display import IFrame\n",
    "import pdfplumber\n",
    "import spacy\n",
    "import PyPDF2\n",
    "import re\n",
    "import torch\n",
    "\n",
    "from collections import Counter\n",
    "from transformers import pipeline\n",
    "\n",
    "# Extract text from PDF using pdfplumber\n",
    "def pdf_to_text_pdfplumber(file_path):\n",
    "    text = \"\"    \n",
    "    with pdfplumber.open(file_path) as pdf:        \n",
    "        for page in pdf.pages:            \n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# Extract text from PDF using PyPDF2\n",
    "def deprecated_pdf_to_text_pypdf2(file_path):\n",
    "    with open(file_path, 'rb') as pdf_file:\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "        text = \"\"\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "        return text\n",
    "\n",
    "def pdf_to_text_pypdf2(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = ''\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() + '\\n'\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "20220021-0a70-4d8f-bb25-9847908c7aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_with_spacy_label(text):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    return [ent.text for ent in doc.ents if ent.label_ in [\"ORG\", \"PRODUCT\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "00a3cb2b-1b0c-4a8a-b39d-57561da5984b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_submission_vicinity(text, limit=10):\n",
    "    lines = text.split('\\n')[:limit]\n",
    "    for line in lines:\n",
    "        if \"submission\" in line.lower():\n",
    "            potential_orgs = find_with_spacy_label(line)\n",
    "            if potential_orgs:\n",
    "                return potential_orgs[0]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "e7924c9f-65f5-407f-96ec-3008cd71115b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_in_footer(text):\n",
    "    footer_pattern = r'(?:Â©|\\(c\\)).*?([A-Z][a-z]+(?:[\\s&]+[A-Z][a-z]+)+)'\n",
    "    match = re.search(footer_pattern, text)\n",
    "    return match.group(1) if match else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "4a6b611d-cac4-41ec-a500-f3e955c5f342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_acronym(text):\n",
    "    lines = text.split('\\n')[:15]\n",
    "    acronym_pattern = r'\\b([A-Z]{2,})\\b'\n",
    "    for line in lines:\n",
    "        match = re.search(acronym_pattern, line)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "78e8b189-a1b9-4494-b065-8cb65c63639f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_in_signature(text):\n",
    "    last_page = text.split('\\n')[-20:]  # Assume signature is in the last 20 lines\n",
    "    signature_pattern = r'(?:Sincerely|Yours truly|Regards),?(?:\\s*\\n)*\\s*([A-Z][a-z]+(?:[\\s&]+[A-Z][a-z]+)+)'\n",
    "    for i in range(len(last_page) - 1):\n",
    "        two_lines = ' '.join(last_page[i:i+2])\n",
    "        match = re.search(signature_pattern, two_lines)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "04b32e73-ba5d-4462-a8bd-21eb34a96045",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_participant(pdf_path):\n",
    "    text = pdf_to_text_pypdf2(pdf_path)\n",
    "    \n",
    "    potential_matches = []\n",
    "    \n",
    "    # Method 1: Organization with \"submission\"\n",
    "    with_submission = find_submission_vicinity(text)\n",
    "    if with_submission:\n",
    "        potential_matches.append(with_submission)\n",
    "    \n",
    "    # Method 2: Organization in footer\n",
    "    in_footer = find_in_footer(text)\n",
    "    if in_footer:\n",
    "        potential_matches.append(in_footer)\n",
    "    \n",
    "    # Method 3: Organization acronym\n",
    "    in_acronym = find_acronym(text)\n",
    "    if in_acronym:\n",
    "        potential_matches.append(in_acronym)\n",
    "    \n",
    "    # Method 4: Organization in signature\n",
    "    in_signature = find_in_signature(text)\n",
    "    if in_signature:\n",
    "        potential_matches.append(in_signature)\n",
    "    \n",
    "    # Method 5: Using NER to find all potential organizations\n",
    "    with_spacy_label = find_with_spacy_label(text)\n",
    "    potential_matches.extend(with_spacy_label)\n",
    "\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Use a pre-trained model for organization entity recognition as a final check\n",
    "    ner_pipeline = pipeline(\"ner\", model=\"dslim/bert-base-NER\", device=device)\n",
    "    ner_results = ner_pipeline(text[:1000])  # Limit to first 1000 characters for efficiency\n",
    "    ner_orgs = [result['word'] for result in ner_results if result['entity'] in ['B-ORG', 'I-ORG']]\n",
    "    potential_matches.extend(ner_orgs)\n",
    "    \n",
    "    # Count occurrences and select the most common organization\n",
    "    match_counts = Counter(potential_matches)\n",
    "    most_common_match = match_counts.most_common(1)[0][0] if match_counts else None\n",
    "    \n",
    "    return most_common_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "312a227a-b345-47c1-8a8b-61c674e98941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Worked\n",
    "#file_path = \"./pdfs/Tenox-Consulting-submission-on-draft-sports-broadcasting-services-amendment-regulations-2018 - 3.pdf\"\n",
    "#file_path = \"./pdfs/South-African-Youth-Council-submission-on-draft-sports-broadcasting-services-amendment-regulations-2018 - 1.pdf\"\n",
    "\n",
    "# Partially Worked\n",
    "# Gave SARU\n",
    "#file_path = \"./pdfs/SARU-submission-on-draft-sports-broadcasting-services-amendment-regulations-2018 - 30.pdf\"\n",
    "#\n",
    "# Failed\n",
    "#file_path = \"./pdfs/UCT-submission-on-draft-sports-broadcasting-services-amendment-regulations-2018 - 5.pdf\" \n",
    "#file_path = \"./pdfs/University-of-Pretoria-submission-on-draft-sports-broadcasting-services-amendment-regulations-2018 - 4.pdf\"\n",
    "#file_path = \"./pdfs/BMI-submission-on-draft-sports-broadcasting-services-amendment-regulations-2018 - 71.pdf\"\n",
    "\n",
    "#file_path = \"your_pdf_file_path\"\n",
    "# IFrame(file_path, width=800, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "77338671-ed7a-4bae-8ef2-6a81a8962f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The extracted participant is: SABC\n"
     ]
    }
   ],
   "source": [
    "participant = find_participant(file_path)\n",
    "print(f\"The extracted participant is: {participant}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5a4e4e-a3ec-4587-b55c-f8764147e794",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(nlp.get_pipe(\"ner\").labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96b37a3-0ed0-49be-8163-77df65c96270",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = pdf_to_text_pdfplumber(file_path)\n",
    "text = pdf_to_text_pypdf2(file_path)\n",
    "# print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c116b608-e624-41f8-ac29-eccba2c44c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess text: remove stop words, convert to lowercase\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "tokens = word_tokenize(text.lower())\n",
    "tokens = [t for t in tokens if t not in stop_words]\n",
    "doc = nlp(\" \".join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9f47bd-b034-4730-b94d-6f23994528cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the text with SpaCy\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef5cdf7-e5dc-4609-9837-555950f75269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Rule-based Extraction: Look for patterns like \"Submitted by:\", \"Prepared by:\"\n",
    "possible_names_1 = []\n",
    "for i, token in enumerate(doc):\n",
    "    if token.text.lower() in (\"submitted\", \"prepared\", \"response\", \"input\"):\n",
    "        if doc[i+1].text.lower() == \"by\":\n",
    "            possible_names_1.append(\" \".join([t.text for t in doc[i+2:i+6] if t.text.isalpha()]))\n",
    "\n",
    "print(possible_names_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e03e7a-9f06-425a-bbfb-88e940a7a632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Extract from Header/Footer\n",
    "possible_names_2 = []\n",
    "for line in text.split(\"\\n\")[:5]:  # Check first 5 lines for header\n",
    "    possible_names_2.extend([ent.text for ent in nlp(line).ents if ent.label_ == \"ORG\"])\n",
    "for line in text.split(\"\\n\")[-5:]: # Check last 5 lines for footer\n",
    "    possible_names_2.extend([ent.text for ent in nlp(line).ents if ent.label_ == \"ORG\"])\n",
    "\n",
    "print(possible_names_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af684b9f-11a6-419c-87ba-a8822d332983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Extract from First Few Lines\n",
    "possible_names_3 = []\n",
    "possible_names_3.extend([ent.text for ent in nlp(\" \".join(text.split(\"\\n\")[:10])).ents if ent.label_ == \"ORG\"])\n",
    "\n",
    "print(possible_names_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484dc187-f7da-4f86-9429-65a6abdcf158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Extract from Signature Section (New - Assumes signature is in last 10 lines)\n",
    "possible_names_4 = []\n",
    "possible_names_4.extend([ent.text for ent in nlp(\" \".join(text.split(\"\\n\")[-10:])).ents if ent.label_ == \"ORG\"])\n",
    "\n",
    "print(possible_names_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34ef5c0-6029-4c6f-b997-0498aa913052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. If no names found using above methods, extract all organizations\n",
    "possible_names_5 = []\n",
    "#if not possible_names:\n",
    "possible_names_5 = [ent.text for ent in doc.ents if ent.label_ == \"ORG\"]\n",
    "\n",
    "print(possible_names_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850bb382-4637-4219-9eee-d611e55c5670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Find the most similar name to the last few lines, first few lines, and the entire document (Modified)\n",
    "if possible_names:\n",
    "    last_lines = \" \".join(text.split(\"\\n\")[-5:])\n",
    "    first_lines = \" \".join(text.split(\"\\n\")[:5:])\n",
    "    most_similar_name = max(possible_names, key=lambda name: \n",
    "                           nlp(name).similarity(nlp(last_lines))\n",
    "                        + nlp(name).similarity(nlp(first_lines))\n",
    "#                        + nlp(name).similarity(nlp(text))) \n",
    "print(most_similar_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b21122-afb4-4e5d-9fa1-f13bf88d44d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
